# GitHub Actions Workflow for Nusantara Food Watch
# Automated daily scraping with email notifications
#
# Setup Instructions:
# 1. Create this file at: .github/workflows/daily_scraper.yml
# 2. Add GitHub Secrets:
#    - DATABASE_URL: Your Supabase connection string
#    - EMAIL_ADDRESS: Gmail address for sending alerts
#    - EMAIL_APP_PASSWORD: Gmail app password (NOT regular password!)
#    - ALERT_EMAIL: Email address to receive alerts
# 3. Push to GitHub
# 4. Check Actions tab to see workflow runs

name: Daily Food Price Scraper

on:
  schedule:
    # Run every day at 6:00 AM Jakarta time (11:00 PM UTC previous day)
    # Cron format: minute hour day month day-of-week
    - cron: '0 23 * * *'
  
  workflow_dispatch:  # Allow manual trigger from GitHub UI
    inputs:
      days_back:
        description: 'Days to scrape back (optional)'
        required: false
        default: '1'

jobs:
  scrape-and-notify:
    runs-on: ubuntu-latest
    
    steps:
    # Step 1: Checkout code
    - name: Checkout repository
      uses: actions/checkout@v3
    
    # Step 2: Setup Python
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'  # Cache pip packages for faster builds
    
    # Step 3: Install dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    # Step 4: Run daily scraper
    - name: Run daily scraper
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
        EMAIL_APP_PASSWORD: ${{ secrets.EMAIL_APP_PASSWORD }}
        ALERT_EMAIL: ${{ secrets.ALERT_EMAIL }}
      run: |
        echo "Starting daily scraper..."
        python daily_scraper.py
      continue-on-error: true  # Continue to notification even if scraper fails
    
    # Step 5: Send notification on success
    - name: Notify success
      if: success()
      env:
        EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
        EMAIL_APP_PASSWORD: ${{ secrets.EMAIL_APP_PASSWORD }}
        ALERT_EMAIL: ${{ secrets.ALERT_EMAIL }}
      run: |
        python -c "
        from src.utils.notifications import send_success_email
        send_success_email()
        "
    
    # Step 6: Send notification on failure
    - name: Notify failure
      if: failure()
      env:
        EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
        EMAIL_APP_PASSWORD: ${{ secrets.EMAIL_APP_PASSWORD }}
        ALERT_EMAIL: ${{ secrets.ALERT_EMAIL }}
      run: |
        python -c "
        from src.utils.notifications import send_failure_email
        send_failure_email('GitHub Actions workflow failed. Check logs.')
        "

# Optional: Add a weekly comprehensive scrape
  weekly-comprehensive-scrape:
    runs-on: ubuntu-latest
    # Run every Sunday at 2 AM Jakarta time
    if: github.event.schedule == '0 19 * * 0'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run weekly comprehensive scrape
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
        EMAIL_APP_PASSWORD: ${{ secrets.EMAIL_APP_PASSWORD }}
        ALERT_EMAIL: ${{ secrets.ALERT_EMAIL }}
      run: |
        echo "Running weekly comprehensive scrape..."
        python daily_scraper.py --days-back 7